## Play mode

Play in the Xcode. The default mode is computer (black) vs human (white). The
board is 8x8, 5 consecutive stones in a row/column/diagonal line wins.

This version has no special rules or bans. And computer alwoyas places the first
stone (black). This can be improved in future.

## Reenforced Learning Mode

Reenforced learning mode, RL mode, does the following steps in a loop.
1. Running self plays with latest checkpoints for 2400 secs. The data generated by this
   round of self plays is called one iteration of the data.
1. Combine the last 20 iterations of data into one single training dataset and
   feed into the CNN. All data are shuffled. And 20% percent of the data is
   held-out data.
1. During the CNN model construction time, before (new) trainig, the previous
   checkpoint (i.e., weights) is loaded and a CoreML model is saved as
   `DistributionLastIteration`. After the training, a new checkpoint is saved
   with the new CoreML model `Distribution`.
1. All weights are backed up with the epoch timestamp in the file names.
1. Loop back.

Before the self plays, there will be rating stage also. The latest checkpoint
will be played against
- the pure random policy
- the MCTS policy with random predictor
- the MCTS policy with previous checkpoint

To execute the RL mode, run

    cd ml
    . automate.sh

The play stats log can be found `/tmp/game-logger.txt` (file will be created
when the stats is available).

## Evaluator Mode

Evaluator mode is running the self plays between the current checkpoint and all
previous checkpoints.

    cd ml
    . evaluator.sh

The play stats log can be found `/tmp/game-logger.txt` (file will be created
when the stats is available).
